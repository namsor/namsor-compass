{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COMPAS Analysis using Aequitas\n",
    "<a id='top_cell'></a>\n",
    "Imagine you have a data set like the COMPAS data set, but protected variables like ethnicity and gender are not given. Can we still test whether bias exists? In this Jupyter Notebook we try to find out. \n",
    "\n",
    "In previous Jupyter Notebooks we edited the cleaned COMPAS data set provided by Aequitas by adding predictions for \n",
    "- sex, using the NamSor API endpoint [parsedGenderGeoBatch](https://v2.namsor.com/NamSorAPIv2/apidoc.html#/personal/parsedGenderGeoBatch), which tries to infer the sex from a first and last name and the country code, in this case 'US'\n",
    "- race, using the NamSor API endpoint [usRaceEthnicity](https://v2.namsor.com/NamSorAPIv2/apidoc.html#/personal/usRaceEthnicity), which tries to infer the ethnicity from a first and last name\n",
    "\n",
    "Even though the `usRaceEthnicity` ethnicity categories do not match COMPAS' race exactly, it should still be useful to show possible unfairness.\n",
    "\n",
    "Using this annotated COMPAS data set, we can now test what happens if we ignore self-reported sex and race. Unlike the original Jupyter Notebook we will be leaving out analysis for the age category, as we did not do any predictions for age.\n",
    "\n",
    "Credits: \n",
    "- We remixed the [Aequitas' cleaned COMPAS data set](https://github.com/dssg/aequitas/blob/master/examples/data/compas_for_aequitas.csv) (originally from ProPublica).\n",
    "- We remixed this Notebook from [Aequitas' Sample Notebook](https://github.com/dssg/aequitas/blob/master/docs/source/examples/compas_demo.ipynb).\n",
    "\n",
    "Content:\n",
    "\n",
    "- [What biases exist in my model?](#existing_biases)\n",
    "    - [What is the distribution of groups, predicted scores, and labels across my dataset?](#xtab)\n",
    "    - [What are bias metrics across groups?](#xtab_metrics)\n",
    "    - [How do I interpret biases in my model?](#interpret_bias)\n",
    "    - [How do I visualize biases in my model?](#bias_viz)\n",
    "\n",
    "- [What levels of disparity exist between population groups?](#disparities)\n",
    "    - [How does the selected reference group affect disparity calculations?](#disparity_calc)\n",
    "    - [How do I interpret calculated disparity ratios?](#interpret_disp)\n",
    "    - [How do I visualize disparities in my model?](#disparity_viz) \n",
    "\n",
    "- [How do I assess model fairness??](#fairness)\n",
    "    - [How do I interpret parities?](#interpret_fairness)\n",
    "    - [How do I visualize bias metric parity?](#fairness_group_viz)\n",
    "    - [How do I visualize parity between groups in my model?](#fairness_disp_viz) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from aequitas.group import Group\n",
    "from aequitas.bias import Bias\n",
    "from aequitas.fairness import Fairness\n",
    "from aequitas.plotting import Plot\n",
    "\n",
    "# import warnings; warnings.simplefilter('ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/namsor_for_aequitas.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We don't need age\n",
    "df = df.drop(['age_cat'], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non String columns will lead to problems later so we have to find out if there are any...\n",
    "non_attr_cols = ['id', 'model_id', 'entity_id', 'score', 'label_value', 'rank_abs', 'rank_pct']\n",
    "attr_cols = df.columns[~df.columns.isin(non_attr_cols)]  # index of the columns that are\n",
    "df.columns[(df.dtypes != object) & (df.dtypes != str) & (df.columns.isin(attr_cols))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and delete them.\n",
    "df = df.drop(['Unnamed: 0'], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration\n",
    "\n",
    "### Risk assessment\n",
    "\n",
    "COMPAS produces a risk score that predicts a person's likelihood of commiting a crime in the next two years. The output is a score between 1 and 10 that maps to low, medium or high. Aequitas collapsed this to a binary prediction. A score of 0 indicates a prediction of \"low\" risk according to COMPAS, while a 1 indicates \"high\" or \"medium\" risk.\n",
    "\n",
    "This categorization is based on ProPublica's interpretation of Northpointe's practioner guide:\n",
    "\n",
    "    \"According to Northpointe’s practitioners guide, COMPAS “scores in the medium and high range \n",
    "    garner more interest from supervision agencies than low scores, as a low score would suggest \n",
    "    there is little risk of general recidivism,” so we considered scores any higher than “low” to \n",
    "    indicate a risk of recidivism.\"\n",
    "\n",
    "The similar obtained withe the NamSor annotated COMPAS data is similar to the one that was obtained by Aequitas for the original data: We see a large difference in how risk scores are distributed by race, with a majority of white and Hispanic people predicted as low risk (score = 0) and a majority of black people predicted high and medium risk (score = 1). We don't see obviousely distinct score distributions by sex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aq_palette = sns.diverging_palette(225, 35, n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_race = sns.countplot(x=\"race\", hue=\"score\", data=df, palette=aq_palette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_sex = sns.countplot(x=\"sex\", hue=\"score\", data=df, palette=aq_palette)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Levels of recidivism\n",
    "\n",
    "This dataset includes information about whether or not the subject recidivated, and so we can directly test the accuracy of the predictions. First, we visualize the recidivsm rates across race. \n",
    "\n",
    "Following ProPublica, we defined recidivism as a new arrest within two years. (If a person recidivates, `label_value` = 1). They \"based this decision on Northpointe’s practitioners guide, which says that its recidivism score is meant to predict 'a new misdemeanor or felony offense within two years of the COMPAS administration date.'\"\n",
    "\n",
    "Again, the results look rather similar to the original graphs, but the key message stays the same: The graphs show the recidivism rates are higher for black defendants compared to white defendants. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_by_race = sns.countplot(x=\"race\", hue=\"label_value\", data=df, palette=aq_palette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_by_sex = sns.countplot(x=\"sex\", hue=\"label_value\", data=df, palette=aq_palette)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#top_cell)\n",
    "\n",
    "## Analysis\n",
    "\n",
    "Do the patterns found in the exploration reflect bias or not. \n",
    "\n",
    "<a id='existing_biases'></a>\n",
    "### Biases\n",
    "\n",
    "We will first be looking at the confusion matrix of each subgroup, calculating commonly used metrics such as false positive rate and false omission rate, as well as counts by group and group prevelance among the sample population. \n",
    "\n",
    "The results we get using NamSor's predictions are a bit different at times than the results we can get from the original data. For instance the tpr for Hispanics is 0.44 originally and 0.52 here. However, for now it appears that the bigger picture is still similar. African-Americans have a fpr of 42%, while Caucasians have a fpr of only 28%. This means that African-American people are far more likely to be falsely labeled as high-risk than white people. The distance between fpr for African-Americans and Caucasians appears to be smaller than for the original data. The distance between False ommision rates (`for`) seem to be even smaller here than in the original data set and the distance between false discovery rates (`fdr`) is even 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Group()\n",
    "xtab, _ = g.get_crosstabs(df)\n",
    "absolute_metrics = g.list_absolute_metrics(xtab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculated counts across sample population groups\n",
    "xtab[[col for col in xtab.columns if col not in absolute_metrics]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculated rates for each sample population group\n",
    "xtab[['attribute_name', 'attribute_value'] + absolute_metrics].round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#top_cell)\n",
    "<a id='bias_viz'></a>\n",
    "\n",
    "#### Visualization\n",
    "\n",
    "The results we obtain when looking at group metrics accross all groups are similar to the original results:\n",
    "\n",
    "FNR: The groups COMPAS incorrectly predicts as 'low' or 'medium' risk most often are Male and African American (see the absolute numbers). From the darker coloring, we can also tell that these are the two largest populations in the data set.\n",
    "\n",
    "PPR: We can see that the largest 'race' group, African Americans, are still predicted positive more often than any other race group (0.56) even though this is less than in the original data set. \n",
    "\n",
    "FPR: African Americans are still more likely to be incorrectly classified as 'high' risk (0.42, a little less than in the original data set) than incorrectly classified as 'low' or 'medium' risk. \n",
    "\n",
    "We can also see that the model is equally likely to incorrectly predict a woman as 'high' risk as it is for a man (false positive rate `FPR` of 0.32 for both Male and Female)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aqp = Plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group metrics accross all groups\n",
    "a = aqp.plot_group_metric_all(xtab, ncols=2, min_group_size=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#top_cell)\n",
    "<a id='disparities'></a>\n",
    "\n",
    "### Disparities between Groups\n",
    "\n",
    "We can observe different results for the NamSor predictions than for the original data.\n",
    "\n",
    "There are almost as many negative predictions for African Americans before but considerably less positive predictions (1863 compared to 2174). For Caucasians the opposite is true: There are now a little more positive predictions and considerably less negative predictions. For Hispanics, there are now way more positive and negative predictions.\n",
    "\n",
    "While there are almost as many incorrect negative predictions and correct negative predictions, the amount of false positives and true negatives is now smaller for African Americans. For Caucasians, the amounts of false negatives, false positives and true positives remained pretty much the same but the amount of true negatives decreased. For Hispanics there are now many more false positives, true positives, false negatives and true negatives with their values roughly doubling. \n",
    "\n",
    "When looking at the prevalence we see that these values stayed roughly the same, but with now a little higher prevalence for Caucasians (0.45 compared to 0.39 before). \n",
    "\n",
    "Let's have a look at changes in disparities resulting from these changes of metric values.\n",
    "\n",
    "From the fpr we still get the result that African Americans are falsely assigned a high or medium risk score more often than Caucasians, but the rate is now 1.4 instead of 1.9. The fdr disparity values changed very little.\n",
    "\n",
    "TODO: Take the important parts and put them into understandable words\n",
    "\n",
    "About true positive rate disparities:\n",
    "* The tpr disparity is now smaller and there is no significant disparity for African American compared to Caucasians.\n",
    "* The tpr disparity is now considerably different for Asians (0.83 compared to 1.28 originally) and there is a significant disparity. This is because NamSor classifies more people as Asian, so the overall group of estimated Asians is bigger in this data set (256 compared to 32). \n",
    "* The tpr disparity is pretty similar for Hispanics in the NamSor annotated data set and in the original data set, but here the disparity is found to be significant. Again, more people are labelled as Hispanic here than in the original data set (1353 compared to 637 previousely).\n",
    "\n",
    "About true negative rate disparities:\n",
    "* Here the results are very similar except that the tnr disparity between Hispanics and Caucasians is significant in this data set. \n",
    "\n",
    "About precision disparities:\n",
    "* The precision disparities here are considerably different from the origin data set, being below 1 here and above 1 in the original data set. Either way, no precision disparity is significant except for the one between Hispanics and Caucasians (in the original data set, no precision disparity was significant at all).\n",
    "\n",
    "About predicted prevalence disparities:\n",
    "* The disparity values are fairly similar, but now the disparity for Asians compared to Caucasians is significant.\n",
    "\n",
    "About positive predictive disparities:\n",
    "* ppr disparity values are rather different. Between African-Americans and Caucasians the disparity was 2.55 originally and is 2 here. Between Hispanics and Caucasians the disparity was 0.22 and is here 0.5. The only change in significance that can be observed is for Asians, where the disparity value is 0.07 here and 0.01 in the original data set.\n",
    "\n",
    "About negative predictive value disparities:\n",
    "* Even though we have similar disparity values except for Hispanics compared to Caucasians where the value was slightly below 1 and is now slightly above 1, we get inversed significance here: The disparity between African-Americans is not significant anymore, but the disparity values for Asians and Hispanics are.\n",
    "\n",
    "About false omission rate disparities:\n",
    "* The for disparity between African-Americans and Caucasians is not significant anymore, it is smaller here.\n",
    "* The for disparity between Asians and Caucasians is now significant and it bigger here.\n",
    "* The for disparity between Hispanics and Caucasians is also significant here and now smaller than 1, while it was slightly above 1 before.\n",
    "\n",
    "About false negative rate disparities:\n",
    "* The fnr disparity for African-Americans is not significant anymore\n",
    "* the fnr disparity for Asians is significant now and bigger than one (1.23) while it was below 1 (0.7) before\n",
    "* the fnr disparity for Hispanics is now significant while the value is still similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = Bias()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bdf = b.get_disparity_predefined_groups(xtab, original_df=df, \n",
    "                                        ref_groups_dict={'race':'Caucasian', 'sex':'Male'}, \n",
    "                                        alpha=0.05, check_significance=True, \n",
    "                                        mask_significance=True)\n",
    "bdf.style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#top_cell)\n",
    "<a id='disparity_viz'></a>\n",
    "\n",
    "#### Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots_race = aqp.plot_disparity_all(bdf, attributes=['race'], metrics = 'all', significance_alpha=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plots_sex = aqp.plot_disparity_all(bdf, attributes=['sex'], metrics = 'all', significance_alpha=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fairness Assessment\n",
    "\n",
    "In this case, our base groups are Caucasian for race and Male for gender. With NamSor annotated data we still get the same result for subervised fairness: Relative to the base groups, the COMPAS predictions only provide supervised fairness to one group, Hispanic.\n",
    "\n",
    "Like in the original data, the African-American false omission and false discovery are within the bounds of fairness. This result is expected because COMPAS is calibrated. (Given calibration, it is surprising that Asian and Native American rates are so low. This may be a matter of having few observations for these groups.)\n",
    "\n",
    "African-Americans are about 1.5 times as likely to have false positives and 40 percent less likely to false negatives. In real terms, 41.5% (a little lower than originally) of African-Americans who did not recidivate were marked high or medium risk (with potential for associated penalties), compared with 28.4% (a little higher than originally) of Caucasian non-reoffenders. This is unfair and is marked False below.\n",
    "\n",
    "Overall, we get the same results for fairness, except:\n",
    "* There is now FDR Parity for race\n",
    "* There is now FPR Parity for sex\n",
    "* There is now TPR Parity for race\n",
    "* There is now TNR Parity for race\n",
    "* There is now Precision Parity for race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = Fairness()\n",
    "fdf = f.get_group_value_fairness(bdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parity_detrminations = f.list_parities(fdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdf[['attribute_name', 'attribute_value'] + absolute_metrics + b.list_disparities(fdf) + parity_detrminations].style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaf = f.get_group_attribute_fairness(fdf)\n",
    "gaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gof = f.get_overall_fairness(fdf)\n",
    "gof"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization\n",
    "We can see that negative predictive parity status is 'True' for all population groups, like with the original data, but we additionally see TPR, TNR and Precision being fair, and that unfairness is smaller for the other values as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg = aqp.plot_fairness_group_all(fdf, ncols=5, metrics = \"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Top](#top_cell)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
